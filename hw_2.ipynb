{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw9TFZlgfD/y/DtqPU9EdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chugcha/hw_proga/blob/main/hw_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Домашнее задание 2**"
      ],
      "metadata": {
        "id": "s4J4b0MjOVbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка книги и всего остального"
      ],
      "metadata": {
        "id": "7KDeV7t7jAh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymystem3 --q\n",
        "!pip install nltk --q\n",
        "!pip install pymorphy3 --q\n",
        "!pip install spacy --q\n",
        "!pip install typing_extensions --upgrade --q\n",
        "!python -m spacy download ru_core_news_sm --q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRIk0CJhSJom",
        "outputId": "bcb72ae2-73fc-4a98-e71d-ad786cf14266"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/15.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/15.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/15.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/15.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m212.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m212.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "DPZ9x-CXOJEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97283d73-739e-487a-e04d-3fd672d989a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-21 22:17:22--  https://raw.githubusercontent.com/chugcha/hw_proga/refs/heads/main/bazhov1tom.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 917244 (896K) [text/plain]\n",
            "Saving to: ‘bazhov1tom.txt’\n",
            "\n",
            "bazhov1tom.txt      100%[===================>] 895.75K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-10-21 22:17:23 (18.5 MB/s) - ‘bazhov1tom.txt’ saved [917244/917244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/chugcha/hw_proga/refs/heads/main/bazhov1tom.txt\n",
        "# Это файл из моего репозитория на гитхабе\n",
        "\n",
        "with open('bazhov1tom.txt', 'r') as file:\n",
        "   book = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK + Pymorphy"
      ],
      "metadata": {
        "id": "EMjJR1x-iv07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "book_tokens = word_tokenize(book, language='russian')\n",
        "sw = stopwords.words('russian')\n",
        "\n",
        "dirty_words = [w.lower() for w in book_tokens if w.isalpha()]\n",
        "clean_words = [w for w in dirty_words if w not in sw]\n",
        "# теперь в clean_words остались только все не-стоп-слова"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBAoFvyPWF5d",
        "outputId": "3444c16b-1a2e-4e69-b4ca-3e75c9835ce5"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from pprint import pprint\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "ana_w = [morph.parse(word) for word in clean_words]\n",
        "# здесь записаны все возможные разборы слов из текста\n",
        "\n",
        "with open('analysed_tokens_pymorphy.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for i in ana_w:\n",
        "        token_data = {\"word\": i[0].word, \"lemma\": i[0].normalized.word, \"pos\": i[0].tag.POS}\n",
        "        f.write(json.dumps(token_data) + '\\n')\n",
        "\n",
        "# теперь всё есть в json файле, смотрите:\n",
        "\n",
        "with open('analysed_tokens_pymorphy.jsonl', 'r', encoding='utf-8') as k:\n",
        "    for i in range(6):\n",
        "        print(json.loads(k.readline()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQXGnrbYZo8",
        "outputId": "458ac67b-6968-4015-8c0a-a1f45b52d252"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'бажов', 'lemma': 'баж', 'pos': 'NOUN'}\n",
            "{'word': 'павел', 'lemma': 'павел', 'pos': 'NOUN'}\n",
            "{'word': 'собрание', 'lemma': 'собрание', 'pos': 'NOUN'}\n",
            "{'word': 'сочинений', 'lemma': 'сочинение', 'pos': 'NOUN'}\n",
            "{'word': 'трёх', 'lemma': 'три', 'pos': 'NUMR'}\n",
            "{'word': 'томах', 'lemma': 'том', 'pos': 'NOUN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#заносим информацию по каждой лемме в словарик для конвертирования в датафрейм\n",
        "data_dict = {'lemmas': [], 'pos': [], 'frequency': []}\n",
        "\n",
        "for i in ana_w:\n",
        "  if i[0].normalized.word not in data_dict['lemmas']: # добавляем уникальную лемму\n",
        "      data_dict['lemmas'].append(i[0].normalized.word)\n",
        "      data_dict['pos'].append(i[0].normalized.tag.POS)\n",
        "      data_dict['frequency'].append(1)\n",
        "\n",
        "  else:\n",
        "      # оказалось, бывает, что одну лемму определяют как разные части речи\n",
        "      if data_dict['pos'][data_dict['lemmas'].index(i[0].normalized.word)] != i[0].normalized.tag.POS:\n",
        "          same_lemmas_POS = []\n",
        "          same_lemmas = [j for j, item in enumerate(data_dict['lemmas']) if item == i[0].normalized.word]\n",
        "          for m in same_lemmas:\n",
        "              same_lemmas_POS.append(data_dict['pos'][m])\n",
        "          if i[0].normalized.tag.POS not in same_lemmas_POS:\n",
        "              data_dict['lemmas'].append(i[0].normalized.word)\n",
        "              data_dict['pos'].append(i[0].normalized.tag.POS)\n",
        "              data_dict['frequency'].append(1)\n",
        "      else:\n",
        "          data_dict['frequency'][data_dict['lemmas'].index(i[0].normalized.word)] += 1\n",
        "# это на всякий случай, проверить, что работает\n",
        "print('Всего лексем:', len(data_dict['lemmas']))\n",
        "print('Абсолютная частота вхождений лексемы \"кошка\":', data_dict['frequency'][data_dict['lemmas'].index('кошка')])\n"
      ],
      "metadata": {
        "id": "gkOmg-UyrVmN",
        "outputId": "b5fbb326-4c23-4d4f-a1a3-3fd6238015f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего лексем: 7836\n",
            "Абсолютная частота вхождений лексемы \"кошка\": 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "bazhov_df = pd.DataFrame(data_dict)\n",
        "print(bazhov_df.info())\n",
        "print('\\n')\n",
        "#видно, что у некоторых слов не хватает каких-то значений POS. Выведем их\n",
        "words_without_pos = bazhov_df[bazhov_df['pos'].isna()]\n",
        "print(f\"Слова без части речи: {len(words_without_pos)}\")\n",
        "print(words_without_pos['lemmas'].to_list())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxWNNk_3Yn0E",
        "outputId": "748d526a-3fd9-48ec-f056-344d41459dfb"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7836 entries, 0 to 7835\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   lemmas     7836 non-null   object\n",
            " 1   pos        7820 non-null   object\n",
            " 2   frequency  7836 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 183.8+ KB\n",
            "None\n",
            "\n",
            "\n",
            "Слова без части речи: 16\n",
            "['постеля', 'пировля', 'ина', 'гож', 'дедко', 'полишку', 'задохся', 'ремки', 'нелзя', 'еси', 'заделья', 'деревнешка', 'гач', 'кго', 'деревнл', 'эку']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df = bazhov_df.sort_values('frequency', ascending=False)\n",
        "print(sorted_df)"
      ],
      "metadata": {
        "id": "9MqfRz36c6Ac",
        "outputId": "fddd8f44-5c44-48fd-9c32-606b28d821af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           lemmas   pos  frequency\n",
            "41          стать  INFN        530\n",
            "122      говорить  INFN        509\n",
            "131           это  PRCL        442\n",
            "295           тот  ADJF        375\n",
            "192          дело  NOUN        311\n",
            "...           ...   ...        ...\n",
            "7818      площадь  NOUN          1\n",
            "7817  понаторкать  INFN          1\n",
            "7816      знаться  INFN          1\n",
            "7815   замкнуться  INFN          1\n",
            "7814     остатный  ADJF          1\n",
            "\n",
            "[7836 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "sorted_df.to_csv('pymorphy_frequency.csv')"
      ],
      "metadata": {
        "id": "cwXZOG5lf5x2"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "r6lzvVrcjLBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.ru.examples import sentences\n",
        "\n",
        "\n",
        "nlp = spacy.load('ru_core_news_sm')"
      ],
      "metadata": {
        "id": "yMwG0CHVjQre"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ну если нельзя, то очистить можно так:\n",
        "\n",
        "cleaned_book = []\n",
        "\n",
        "doc_sad = nlp(book)\n",
        "for token in doc_sad:\n",
        "  if not token.is_stop and not token.is_punct and not token.is_space:\n",
        "      cleaned_book.append(token.text)\n",
        "doc_happy = nlp(' '.join(clean_words))"
      ],
      "metadata": {
        "id": "ldt2FaCArAWr"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_book[:10])"
      ],
      "metadata": {
        "id": "_cwaoBlZxVpD",
        "outputId": "efb641ad-9eaa-46ec-8a1e-7103d7d3a3b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Бажов', 'Павел', 'Собрание', 'сочинений', 'трех', 'Сказки', '1', 'МЕДНОЙ', 'ГОРЫ', 'ХОЗЯЙКА']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# так же, как было с pymorphy и nltk\n",
        "with open('analysed_tokens_spacy.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for token in doc_happy:\n",
        "        token_data = {\"word\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_}\n",
        "        f.write(json.dumps(token_data) + '\\n')\n",
        "\n",
        "# вроде бы всё на месте\n",
        "with open('analysed_tokens_spacy.jsonl', 'r', encoding='utf-8') as k:\n",
        "    for i in range(6):\n",
        "        print(json.loads(k.readline()))"
      ],
      "metadata": {
        "id": "kwPkkO7ovBrs",
        "outputId": "04e5b330-26ed-49de-ee46-6ddcfd352811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'бажов', 'lemma': 'бажов', 'pos': 'NOUN'}\n",
            "{'word': 'павел', 'lemma': 'павел', 'pos': 'VERB'}\n",
            "{'word': 'собрание', 'lemma': 'собрание', 'pos': 'NOUN'}\n",
            "{'word': 'сочинений', 'lemma': 'сочинение', 'pos': 'NOUN'}\n",
            "{'word': 'трех', 'lemma': 'три', 'pos': 'NUM'}\n",
            "{'word': 'томах', 'lemma': 'том', 'pos': 'NOUN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# делаем всё аналогично с pymorph\n",
        "data_dict_sp = {'lemmas': [], 'pos': [], 'frequency': []}\n",
        "\n",
        "for token in doc_happy:\n",
        "  if token.lemma_ not in data_dict_sp['lemmas']: # добавляем уникальную лемму\n",
        "      data_dict_sp['lemmas'].append(token.lemma_)\n",
        "      data_dict_sp['pos'].append(token.pos_)\n",
        "      data_dict_sp['frequency'].append(1)\n",
        "\n",
        "  else:\n",
        "      # оказалось, бывает, что одну лемму определяют как разные части речи\n",
        "      if data_dict_sp['pos'][data_dict_sp['lemmas'].index(token.lemma_)] != token.pos_:\n",
        "          same_lemmas_POS = []\n",
        "          same_lemmas = [j for j, item in enumerate(data_dict_sp['lemmas']) if item == token.lemma_]\n",
        "          for m in same_lemmas:\n",
        "              same_lemmas_POS.append(data_dict_sp['pos'][m])\n",
        "          if token.pos_ not in same_lemmas_POS:\n",
        "              data_dict_sp['lemmas'].append(token.lemma_)\n",
        "              data_dict_sp['pos'].append(token.pos_)\n",
        "              data_dict_sp['frequency'].append(1)\n",
        "      else:\n",
        "          data_dict_sp['frequency'][data_dict_sp['lemmas'].index(token.lemma_)] += 1\n",
        "# это на всякий случай, проверить, что работает\n",
        "print('Всего лексем:', len(data_dict_sp['lemmas']))\n",
        "print('Абсолютная частота вхождений лексемы \"кошка\":', data_dict_sp['frequency'][data_dict_sp['lemmas'].index('кошка')])\n"
      ],
      "metadata": {
        "id": "SJZvK7CjyIoT",
        "outputId": "5662f564-c5df-4a8d-af0f-c3f01f4a5a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего лексем: 11623\n",
            "Абсолютная частота вхождений лексемы \"кошка\": 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bazhov_sp = pd.DataFrame(data_dict_sp)\n",
        "print(bazhov_sp.info(), '\\n')\n",
        "\n",
        "sorted_df_sp = bazhov_sp.sort_values('frequency', ascending=False)\n",
        "print('А вот и частотность: \\n')\n",
        "print(sorted_df_sp)\n",
        "\n",
        "sorted_df_sp.to_csv('spacy_frequency.csv')"
      ],
      "metadata": {
        "id": "yuCWa3CD0d6z",
        "outputId": "99bd3fd1-c862-4bb9-83c3-0eb0516db47f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11623 entries, 0 to 11622\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   lemmas     11623 non-null  object\n",
            " 1   pos        11623 non-null  object\n",
            " 2   frequency  11623 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 272.5+ KB\n",
            "None \n",
            "\n",
            "А вот и частотность: \n",
            "\n",
            "            lemmas   pos  frequency\n",
            "42           стать  VERB        504\n",
            "125       говорить  VERB        499\n",
            "311            тот   DET        353\n",
            "134            это  PRON        338\n",
            "107          место  NOUN        290\n",
            "...            ...   ...        ...\n",
            "20     праздничный  VERB          1\n",
            "11607       угадаю  VERB          1\n",
            "11608    болботать   ADV          1\n",
            "11609      одинова   ADJ          1\n",
            "11594   огромадная   ADJ          1\n",
            "\n",
            "[11623 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ]
}