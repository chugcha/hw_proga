{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN03RffUbKly3gfpgaNncFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chugcha/hw_proga/blob/main/hw_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Домашнее задание 2**"
      ],
      "metadata": {
        "id": "s4J4b0MjOVbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка книги и всего остального"
      ],
      "metadata": {
        "id": "7KDeV7t7jAh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymystem3 --q\n",
        "!pip install nltk --q\n",
        "!pip install pymorphy3 --q\n",
        "!pip install spacy --q\n",
        "!pip install typing_extensions --upgrade --q\n",
        "!python -m spacy download ru_core_news_sm --q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRIk0CJhSJom",
        "outputId": "bdf278a2-9e95-4a5a-d104-a858c131cd5c"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "DPZ9x-CXOJEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf212dc-91c4-4992-c9e0-a698a5608caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-24 20:59:14--  https://raw.githubusercontent.com/chugcha/hw_proga/refs/heads/main/bazhov1tom.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 917244 (896K) [text/plain]\n",
            "Saving to: ‘bazhov1tom.txt.1’\n",
            "\n",
            "bazhov1tom.txt.1    100%[===================>] 895.75K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-10-24 20:59:14 (18.3 MB/s) - ‘bazhov1tom.txt.1’ saved [917244/917244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/chugcha/hw_proga/refs/heads/main/bazhov1tom.txt\n",
        "# Это файл из моего репозитория на гитхабе\n",
        "\n",
        "with open('bazhov1tom.txt', 'r') as file:\n",
        "   book_not_lower = file.read()\n",
        "   book = book_not_lower.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK + Pymorphy"
      ],
      "metadata": {
        "id": "EMjJR1x-iv07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "book_tokens = word_tokenize(book, language='russian')\n",
        "sw = stopwords.words('russian')\n",
        "\n",
        "dirty_words = [w.lower() for w in book_tokens if w.isalpha()]\n",
        "clean_words = [w for w in dirty_words if w not in sw]\n",
        "# теперь в clean_words остались только все не-стоп-слова"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBAoFvyPWF5d",
        "outputId": "789e1380-0ed7-4e8e-e072-bd0ff75e74fd"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from pprint import pprint\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "ana_w = [morph.parse(word) for word in clean_words]\n",
        "# здесь записаны все возможные разборы слов из текста\n",
        "\n",
        "with open('analysed_tokens_pymorphy.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for i in ana_w:\n",
        "        token_data = {\"word\": i[0].word, \"lemma\": i[0].normalized.word, \"pos\": i[0].tag.POS}\n",
        "        f.write(json.dumps(token_data) + '\\n')\n",
        "\n",
        "# теперь всё есть в json файле, смотрите:\n",
        "\n",
        "with open('analysed_tokens_pymorphy.jsonl', 'r', encoding='utf-8') as k:\n",
        "    for i in range(6):\n",
        "        print(json.loads(k.readline()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQXGnrbYZo8",
        "outputId": "1d8492be-9068-4a37-f062-299ee55adac3"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'бажов', 'lemma': 'баж', 'pos': 'NOUN'}\n",
            "{'word': 'павел', 'lemma': 'павел', 'pos': 'NOUN'}\n",
            "{'word': 'собрание', 'lemma': 'собрание', 'pos': 'NOUN'}\n",
            "{'word': 'сочинений', 'lemma': 'сочинение', 'pos': 'NOUN'}\n",
            "{'word': 'трёх', 'lemma': 'три', 'pos': 'NUMR'}\n",
            "{'word': 'томах', 'lemma': 'том', 'pos': 'NOUN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#заносим информацию по каждой лемме в словарик для конвертирования в датафрейм\n",
        "data_dict = {'lemmas': [], 'pos': [], 'frequency': []}\n",
        "\n",
        "for i in ana_w:\n",
        "  if i[0].normalized.word not in data_dict['lemmas']: # добавляем уникальную лемму\n",
        "      data_dict['lemmas'].append(i[0].normalized.word)\n",
        "      data_dict['pos'].append(i[0].normalized.tag.POS)\n",
        "      data_dict['frequency'].append(1)\n",
        "\n",
        "  else:\n",
        "      # оказалось, бывает, что одну лемму определяют как разные части речи\n",
        "      if data_dict['pos'][data_dict['lemmas'].index(i[0].normalized.word)] != i[0].normalized.tag.POS:\n",
        "          same_lemmas_POS = []\n",
        "          same_lemmas = [j for j, item in enumerate(data_dict['lemmas']) if item == i[0].normalized.word]\n",
        "          for m in same_lemmas:\n",
        "              same_lemmas_POS.append(data_dict['pos'][m])\n",
        "          if i[0].normalized.tag.POS not in same_lemmas_POS:\n",
        "              data_dict['lemmas'].append(i[0].normalized.word)\n",
        "              data_dict['pos'].append(i[0].normalized.tag.POS)\n",
        "              data_dict['frequency'].append(1)\n",
        "      else:\n",
        "          data_dict['frequency'][data_dict['lemmas'].index(i[0].normalized.word)] += 1\n",
        "# это на всякий случай, проверить, что работает\n",
        "print('Всего лексем:', len(data_dict['lemmas']))\n",
        "print('Абсолютная частота вхождений лексемы \"кошка\":', data_dict['frequency'][data_dict['lemmas'].index('кошка')])\n"
      ],
      "metadata": {
        "id": "gkOmg-UyrVmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "bazhov_df = pd.DataFrame(data_dict)\n",
        "print(bazhov_df.info())\n",
        "print('\\n')\n",
        "#видно, что у некоторых слов не хватает каких-то значений POS. Выведем их\n",
        "words_without_pos = bazhov_df[bazhov_df['pos'].isna()]\n",
        "print(f\"Слова без части речи: {len(words_without_pos)}\")\n",
        "print(words_without_pos['lemmas'].to_list())\n"
      ],
      "metadata": {
        "id": "oxWNNk_3Yn0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df = bazhov_df.sort_values('frequency', ascending=False)\n",
        "print(sorted_df)"
      ],
      "metadata": {
        "id": "9MqfRz36c6Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "sorted_df.to_csv('pymorphy_frequency.csv')"
      ],
      "metadata": {
        "id": "cwXZOG5lf5x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "r6lzvVrcjLBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.ru.examples import sentences\n",
        "\n",
        "\n",
        "nlp = spacy.load('ru_core_news_sm')"
      ],
      "metadata": {
        "id": "yMwG0CHVjQre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# очищаем текст от всего ненужного\n",
        "\n",
        "cleaned_book = []\n",
        "doc_sad = nlp(book)\n",
        "for token in doc_sad:\n",
        "  if not token.is_stop and not token.is_punct and not token.is_space and not token.is_digit:\n",
        "      cleaned_book.append(token.text)\n",
        "doc_happy = nlp(' '.join(clean_words))"
      ],
      "metadata": {
        "id": "ldt2FaCArAWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_book[:10])"
      ],
      "metadata": {
        "id": "_cwaoBlZxVpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# так же, как было с pymorphy и nltk\n",
        "with open('analysed_tokens_spacy.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for token in doc_happy:\n",
        "        token_data = {\"word\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_}\n",
        "        f.write(json.dumps(token_data) + '\\n')\n",
        "\n",
        "# вроде бы всё на месте\n",
        "with open('analysed_tokens_spacy.jsonl', 'r', encoding='utf-8') as k:\n",
        "    for i in range(6):\n",
        "        print(json.loads(k.readline()))"
      ],
      "metadata": {
        "id": "kwPkkO7ovBrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# делаем всё аналогично с pymorph\n",
        "data_dict_sp = {'lemmas': [], 'pos': [], 'frequency': []}\n",
        "\n",
        "for token in doc_happy:\n",
        "  if token.lemma_ not in data_dict_sp['lemmas']: # добавляем уникальную лемму\n",
        "      data_dict_sp['lemmas'].append(token.lemma_)\n",
        "      data_dict_sp['pos'].append(token.pos_)\n",
        "      data_dict_sp['frequency'].append(1)\n",
        "\n",
        "  else:\n",
        "      # оказалось, бывает, что одну лемму определяют как разные части речи\n",
        "      if data_dict_sp['pos'][data_dict_sp['lemmas'].index(token.lemma_)] != token.pos_:\n",
        "          same_lemmas_POS = []\n",
        "          same_lemmas = [j for j, item in enumerate(data_dict_sp['lemmas']) if item == token.lemma_]\n",
        "          for m in same_lemmas:\n",
        "              same_lemmas_POS.append(data_dict_sp['pos'][m])\n",
        "          if token.pos_ not in same_lemmas_POS:\n",
        "              data_dict_sp['lemmas'].append(token.lemma_)\n",
        "              data_dict_sp['pos'].append(token.pos_)\n",
        "              data_dict_sp['frequency'].append(1)\n",
        "      else:\n",
        "          data_dict_sp['frequency'][data_dict_sp['lemmas'].index(token.lemma_)] += 1\n",
        "# это на всякий случай, проверить, что работает\n",
        "print('Всего лексем:', len(data_dict_sp['lemmas']))\n",
        "print('Абсолютная частота вхождений лексемы \"кошка\":', data_dict_sp['frequency'][data_dict_sp['lemmas'].index('кошка')])\n"
      ],
      "metadata": {
        "id": "SJZvK7CjyIoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bazhov_sp = pd.DataFrame(data_dict_sp)\n",
        "print(bazhov_sp.info(), '\\n')\n",
        "\n",
        "sorted_df_sp = bazhov_sp.sort_values('frequency', ascending=False)\n",
        "print('А вот и частотность: \\n')\n",
        "print(sorted_df_sp)\n",
        "\n",
        "sorted_df_sp.to_csv('spacy_frequency.csv')"
      ],
      "metadata": {
        "id": "yuCWa3CD0d6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ частотности морфем"
      ],
      "metadata": {
        "id": "RXbKV9W-cvIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Количество лексем из анализа pymorphy: ', bazhov_df.shape[0])\n",
        "print('Количество лексем из анализа spacy: ', bazhov_sp.shape[0], '\\n')\n",
        "\n",
        "print('Набор морфем pymorph: ', bazhov_df[\"pos\"].unique(), '\\n')\n",
        "print('Набор морфем spacy: ', bazhov_sp[\"pos\"].unique())"
      ],
      "metadata": {
        "id": "CAsmjuX-c2rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получились некрасивые наборы, приведём их к общему виду:\n",
        "def rename_pos_pymorphy(pos):\n",
        "    if pos is None:\n",
        "      return 'X'\n",
        "    elif pos == 'NUMR':\n",
        "        return 'NUM'\n",
        "    elif pos == 'ADJF' or pos == 'ADJS':\n",
        "        return 'ADJ'\n",
        "    elif pos == 'NPRO':\n",
        "        return 'PRON'\n",
        "    elif pos == 'INFN':\n",
        "        return 'VERB'\n",
        "    elif pos == 'ADVB':\n",
        "        return 'ADV'\n",
        "    elif pos == 'PRCL':\n",
        "        return 'PART'\n",
        "    else:\n",
        "        return pos\n",
        "\n",
        "def rename_pos_spacy(pos):\n",
        "    if pos == 'CCONJ' or pos == 'SCONJ':\n",
        "        return 'CONJ'\n",
        "    elif pos == 'PROPN':\n",
        "        return 'NOUN'\n",
        "    elif pos == 'ADP':\n",
        "        return 'PREP'\n",
        "    elif pos == 'AUX':\n",
        "        return 'PRED'\n",
        "    elif pos == 'DET':\n",
        "        return 'ADJ'\n",
        "    else:\n",
        "        return pos\n",
        "\n",
        "bazhov_df[\"pos\"] = bazhov_df[\"pos\"].apply(rename_pos_pymorphy)\n",
        "bazhov_sp[\"pos\"] = bazhov_sp[\"pos\"].apply(rename_pos_spacy)\n",
        "\n",
        "print('Новый набор морфем pymorph: ', bazhov_df[\"pos\"].unique(), '\\n')\n",
        "print('Новый набор морфем spacy: ', bazhov_sp[\"pos\"].unique())\n",
        "\n",
        "# Получилось, что у нас в pymprph также выделены междометия,\n",
        "# которые в spacy были удалены как стоп-слова. Но я не хочу исключать их из анализа и оставлю."
      ],
      "metadata": {
        "id": "4eSL_BdkmR8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_frec_pm(whatever):\n",
        "   return (int(whatever) / len(clean_words))\n",
        "\n",
        "def count_frec_sp(whatever):\n",
        "   return (int(whatever) / len(cleaned_book))\n",
        "\n",
        "morph_freq_pm = bazhov_df.groupby('pos')['frequency'].sum()\n",
        "morph_freq_sp = bazhov_sp.groupby('pos')['frequency'].sum()\n",
        "morph_freq_pm = morph_freq_pm.apply(count_frec_pm).sort_values(ascending=False)\n",
        "morph_freq_sp = morph_freq_sp.apply(count_frec_sp).sort_values(ascending=False)\n",
        "morph_freq_pm = morph_freq_pm.round(5)\n",
        "morph_freq_sp = morph_freq_sp.round(5)\n",
        "\n",
        "print('Доля частей речи по pymorph:')\n",
        "print(morph_freq_pm, '\\n')\n",
        "print('Доля частей речи по spacy:')\n",
        "print(morph_freq_sp)"
      ],
      "metadata": {
        "id": "tB-x_iz-oi7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По этим данным можно сделать несколько выводов. Во-первых, количество выделенных сушествительных, глаголов, прилагательных и наречий примерно равно, наибольшее сходство есть в частотности глаголов. Та разница, которая видна в доле существительных, прилагательных и местоимений, вероятно, возникла из-за того, что pymorph оказался менее чувствительным к формам, которые spacy смог определить, либо из-за того, что spacy определил их как существительные, что объясняет их большую долю в анализе spacy. То, что spacy выделяет большее количество местоимений, объясняется особенностью фильтрации стоп-слов, которые он применял."
      ],
      "metadata": {
        "id": "YDWN5Ptl6pGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визуализация 1"
      ],
      "metadata": {
        "id": "wqJ8vWIeyTag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZCCIQGQeyW2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pm = morph_freq_pm.index.tolist()\n",
        "Y_pm = morph_freq_pm.values.tolist()\n",
        "\n",
        "X_sp = morph_freq_sp.index.tolist()\n",
        "Y_sp = morph_freq_sp.values.tolist()\n",
        "\n",
        "print('Pymorphy: \\n', X_pm, '\\n', Y_pm, '\\n')\n",
        "print('Spacy: \\n', X_sp, '\\n', Y_sp)"
      ],
      "metadata": {
        "id": "p3rRqtazzAIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5), dpi=300)\\\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('По анализу Pymorph')\n",
        "plt.ylabel('Относительная частотность')\n",
        "plt.xlabel('Часть речи')\n",
        "plt.bar(X_pm, Y_pm, color='hotpink')\n",
        "plt.xticks(ticks=X_pm, labels=X_pm)\n",
        "plt.ylim(0, 0.45)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('По анализу Spacy')\n",
        "plt.xlabel('Часть речи')\n",
        "plt.bar(X_sp, Y_sp, color='gold')\n",
        "plt.xticks(ticks=X_sp, labels=X_sp)\n",
        "plt.ylim(0, 0.45)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X4ocb81B02uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ самых частотных частей речи"
      ],
      "metadata": {
        "id": "HKkKFKvF88mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_verbs_pm = bazhov_df[bazhov_df['pos'].isin(['VERB', 'INFN'])].groupby('lemmas')['frequency'].sum()\n",
        "top20_verbs_pm = frequent_verbs_pm.sort_values(ascending=False).head(20)\n",
        "\n",
        "frequent_adv_pm = bazhov_df[bazhov_df['pos'] == 'ADVB'].groupby('lemmas')['frequency'].sum()\n",
        "top20_adv_pm = frequent_adv_pm.sort_values(ascending=False).head(20)\n",
        "\n",
        "verbs_sp = bazhov_sp[bazhov_sp['pos'] == 'VERB'].groupby('lemmas')['frequency'].sum()\n",
        "top20_verbs_sp = verbs_sp.sort_values(ascending=False).head(20)\n",
        "\n",
        "adv_sp = bazhov_sp[bazhov_sp['pos'] == 'ADV'].groupby('lemmas')['frequency'].sum()\n",
        "top20_adv_sp = adv_sp.sort_values(ascending=False).head(20)"
      ],
      "metadata": {
        "id": "um3lheei9uCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top20_verbs_pm.head()"
      ],
      "metadata": {
        "id": "1wzPDgNZ_S_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top20_verbs_sp.head()"
      ],
      "metadata": {
        "id": "9VVfMuh9_fPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top20_adv_sp.head()"
      ],
      "metadata": {
        "id": "teAXalWiAOCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top20_adv_pm.head()"
      ],
      "metadata": {
        "id": "pnq4AuNU_z-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}